import streamlit as st
import numpy as np
import feedparser
import joblib
import json
import re
import sys
from datetime import datetime, timedelta
import dateutil.parser
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
import requests
from bs4 import BeautifulSoup
from pymorphy3 import MorphAnalyzer

sys.path.insert(0, str(Path(__file__).resolve().parent.parent.parent))
from config.urls import RSS_FEEDS
from config.processing_config import CLEANING_PATTERNS

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title="–ù–æ–≤–æ—Å—Ç–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä", layout="wide")
st.title("üì∞ –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ —Ä—É–±—Ä–∏–∫–µ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π")

# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º –∏ –º–∞–ø–ø–∏–Ω–≥—É –∫–∞—Ç–µ–≥–æ—Ä–∏–π
BASE_DIR = Path(__file__).resolve().parent.parent.parent
BASELINE_MODEL_DIR = BASE_DIR / "models" / "baseline_models"
NEURAL_MODEL_DIR = BASE_DIR / "models" / "neural_models"
LABEL_MAP_PATH = BASE_DIR / "data" / "processed" / "label_map.json"
TRAIN_NEURAL_PATH = BASE_DIR / "scr" / "models" / "train_neural"

# –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å "svm", "lr", "lgbm", "fnn", "cnn", "rnn"
DEFAULT_MODEL_NAME = "fnn"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–∞–ø–ø–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
if LABEL_MAP_PATH.exists():
    with open(LABEL_MAP_PATH, "r", encoding="utf-8") as f:
        label_map = json.load(f)
    categories_to_idx = {v: int(k) for k, v in label_map.items()}
    categories = list(categories_to_idx.keys())
else:
    st.error("–§–∞–π–ª label_map.json –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç—å.")
    st.stop()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
st.sidebar.header("–ù–∞—Å—Ç—Ä–æ–π–∫–∏")

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
available_models = []
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏
for pkl_file in BASELINE_MODEL_DIR.glob("model_*.pkl"):
    model_name = pkl_file.stem.replace("model_", "")
    available_models.append(f"{model_name} (classical)")
# –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
if NEURAL_MODEL_DIR.exists():
    for model_dir in NEURAL_MODEL_DIR.iterdir():
        if model_dir.is_dir() and (model_dir / "model.pt").exists():
            available_models.append(f"{model_dir.name} (neural)")

if not available_models:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π!")
    st.stop()

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
default_model_idx = 0
for i, model_name in enumerate(available_models):
    if DEFAULT_MODEL_NAME in model_name:
        default_model_idx = i
        break

selected_model_str = st.sidebar.selectbox(
    "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å",
    available_models,
    index=default_model_idx
)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –º–æ–¥–µ–ª–∏ –∏ —Ç–∏–ø
if " (classical)" in selected_model_str:
    selected_model_name = selected_model_str.replace(" (classical)", "")
    selected_model_type = "classical"
else:
    selected_model_name = selected_model_str.replace(" (neural)", "")
    selected_model_type = "neural"

selected_category = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ —Ä—É–±—Ä–∏–∫—É", categories)

period_options = {
    "–ó–∞ –≤—Å—ë –≤—Ä–µ–º—è": None,
    "–ü–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å": 1,
    "–ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è": 3,
    "–ü–æ—Å–ª–µ–¥–Ω—è—è –Ω–µ–¥–µ–ª—è": 7,
    "–ü–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü": 30
}
selected_period = st.sidebar.selectbox("–ü–µ—Ä–∏–æ–¥ –Ω–æ–≤–æ—Å—Ç–µ–π", list(period_options.keys()))

# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞ –æ–¥–∏–Ω –∑–∞–ø—É—Å–∫
MAX_RESULTS = 20

def get_feed_cache() -> dict:
    if "feed_cache" not in st.session_state:
        st.session_state["feed_cache"] = {}
    return st.session_state["feed_cache"]

def get_seen_items() -> set:
    if "seen_items" not in st.session_state:
        st.session_state["seen_items"] = set()
    return st.session_state["seen_items"]

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
@st.cache_resource
def load_model_wrapper(model_name: str, model_type: str):
    from scr.models.model_loader import load_model_by_name
    
    device = "cpu"
    return load_model_by_name(
        model_name=model_name,
        base_dir=BASE_DIR,
        device=device,
        train_neural_path=TRAIN_NEURAL_PATH
    )

@st.cache_resource
def load_lemmatizer():
    return MorphAnalyzer()

@st.cache_resource
def get_lemma_cache():
    return {}

def preprocess_text(text: str, morph: MorphAnalyzer) -> str:
    text = re.sub(CLEANING_PATTERNS['tags'], '', text)
    text = re.sub(CLEANING_PATTERNS['urls'], '', text)
    text = re.sub(CLEANING_PATTERNS['non_alpha'], '', text)
    text = text.lower()
    text = re.sub(CLEANING_PATTERNS['extra_spaces'], ' ', text).strip()

    words = text.split()

    lemma_cache = get_lemma_cache()

    lemmas = []
    for word in words:
        if word not in lemma_cache:
            lemma_cache[word] = morph.parse(word)[0].normal_form
        lemmas.append(lemma_cache[word])

    return ' '.join(lemmas)

def parse_feed_with_timeout(feed_url: str, timeout: int = 10):
    feed_cache = get_feed_cache()
    cache_entry = feed_cache.get(feed_url)

    try:
        response = requests.get(feed_url, timeout=timeout, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        content = response.content
        checksum = hashlib.md5(content).hexdigest()

        # –ï—Å–ª–∏ –≤ –∫–µ—à–µ –µ—Å—Ç—å —Ç–æ—Ç –∂–µ —Å–∞–º—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if cache_entry and cache_entry["checksum"] == checksum:
            cache_entry["timestamp"] = datetime.now()
            return cache_entry["entries"], True

        parsed = feedparser.parse(content)
        entries = []
        for entry in parsed.entries:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'published' in entry:
                try:
                    dt = dateutil.parser.parse(entry['published'])
                    entry['datetime'] = dt.replace(tzinfo=None)
                except Exception:
                    entry['datetime'] = None
            else:
                entry['datetime'] = None
            entries.append(entry)

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
        feed_cache[feed_url] = {
            "checksum": checksum,
            "entries": entries,
            "timestamp": datetime.now(),
        }
        return entries, False

    except requests.exceptions.Timeout:
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—Ç–∞—Ä—ã–π –∫–µ—à - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ, –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        if cache_entry:
            return cache_entry["entries"], True
        return [], False
    except Exception:
        if cache_entry:
            return cache_entry["entries"], True
        return [], False

def filter_entries_by_period(entries, period_name):
    days = period_options[period_name]
    if days is None:
        return entries
    cutoff = datetime.now() - timedelta(days=days)
    filtered = []
    for e in entries:
        dt = e.get('datetime')
        if dt and dt >= cutoff:
            filtered.append(e)
    return filtered

def extract_clean_description(html_text: str) -> str:
    if not html_text:
        return ""

    soup = BeautifulSoup(html_text, "html.parser")

    # –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –∞–±–∑–∞—Ü—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
    paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
    if paragraphs:
        return "\n\n".join(paragraphs)

    # –í –ø—Ä–æ—Ç–∏–≤–Ω–æ–º —Å–ª—É—á–∞–µ ‚Äî –ø—Ä–æ—Å—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç
    return soup.get_text(" ", strip=True)

def classify_entries_for_feed(feed_url: str, selected_period_name: str, target_idx: int,
                              model_wrapper, morph):
    entries, _ = parse_feed_with_timeout(feed_url)
    if not entries:
        return []

    period_filtered = filter_entries_by_period(entries, selected_period_name)
    if not period_filtered:
        return []

    results = []
    for entry in period_filtered:
        title = entry.get("title", "")
        raw_description = entry.get("description", "")

        display_description = extract_clean_description(raw_description)

        if not title or not display_description or not display_description.strip():
            continue

        raw_text_for_model = f"{title} {display_description}"
        processed = preprocess_text(raw_text_for_model, morph)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ModelWrapper
        pred_idx = model_wrapper.predict(processed)[0]

        if pred_idx == target_idx:
            # –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –≤–∏–¥–µ —á–∏—Å–ª–∞ [0, 1]
            proba = model_wrapper.predict_proba(processed)[0]
            confidence_score = float(proba[pred_idx])

            if confidence_score < 0.4:
                continue

            confidence_display = f"{confidence_score * 100.0:.1f}%"

            results.append({
                "id": f"{feed_url}|{entry.get('link', '')}|{title}",
                "title": title,
                "description": display_description,
                "link": entry.get("link", ""),
                "date": entry.get("published", ""),
                "datetime": entry.get("datetime"),
                "confidence": confidence_display,
                "confidence_score": confidence_score,
                "feed_url": feed_url,
            })

    return results

def render_new_cards(results, container):
    seen_items = get_seen_items()
    new_items = [r for r in results if r["id"] not in seen_items]
    if not new_items:
        return

    for r in new_items:
        seen_items.add(r["id"])
        with container:
            st.markdown(f"### [{r['title']}]({r['link']})")
            if r['description']:
                st.write(r['description'])
            col_a, col_b = st.columns(2)
            date_str = r['datetime'].strftime("%d.%m.%Y %H:%M") if r.get('datetime') else "–î–∞—Ç–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"
            col_a.write(f"üìÖ {date_str}")
            col_b.write(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {r['confidence']}")
            st.divider()

# –û—Å–Ω–æ–≤–Ω–æ–π –±–ª–æ–∫: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–∂–∞—Ç–∏—è –∫–Ω–æ–ø–∫–∏
if st.sidebar.button("–ü–æ–∫–∞–∑–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏"):
    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∑–∞–ø—É—Å–∫–∞
    st.session_state["seen_items"] = set()
    st.session_state["filtered_results"] = []

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∑–∞–≥—Ä—É–∑—á–∏–∫
    model_wrapper = load_model_wrapper(selected_model_name, selected_model_type)
    morph = load_lemmatizer()
    target_idx = categories_to_idx[selected_category]

    progress_bar = st.sidebar.progress(0)
    status_text = st.sidebar.empty()

    # –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è "–ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ" –≤—ã–≤–æ–¥–∞ –∫–∞—Ä—Ç–æ—á–µ–∫
    stream_container = st.container()

    all_results = []

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∏–¥—ã, –Ω–æ UI –æ–±–Ω–æ–≤–ª—è–µ–º –ø–æ –º–µ—Ä–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ
    with ThreadPoolExecutor(max_workers=min(4, len(RSS_FEEDS))) as executor:
        future_to_feed = {
            executor.submit(
                classify_entries_for_feed,
                feed_url,
                selected_period,
                target_idx,
                model_wrapper,
                morph,
            ): feed_url
            for feed_url in RSS_FEEDS
        }

        total = len(future_to_feed)
        completed = 0

        for future in as_completed(future_to_feed):
            feed_url = future_to_feed[future]
            completed += 1

            try:
                feed_results = future.result()
            except Exception as e:
                st.sidebar.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {feed_url}: {e}")
                feed_results = []

            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {completed}/{total} –ª–µ–Ω—Ç")
            progress_bar.progress(completed / total)

            if len(all_results) >= MAX_RESULTS:
                continue

            status_text.text(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {completed}/{total} –ª–µ–Ω—Ç")
            progress_bar.progress(completed / total)

            if feed_results and len(all_results) < MAX_RESULTS:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫ –∏ —Å—Ä–∞–∑—É –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏,
                # –ø–æ–∫–∞ –Ω–µ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç MAX_RESULTS
                for item in feed_results:
                    if len(all_results) >= MAX_RESULTS:
                        break
                    all_results.append(item)
                    render_new_cards([item], stream_container)

    progress_bar.empty()
    status_text.empty()

    st.session_state.filtered_results = all_results[:MAX_RESULTS]

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
if "filtered_results" in st.session_state and st.session_state.filtered_results:
    results = st.session_state.filtered_results
    st.success(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ —Ä—É–±—Ä–∏–∫–µ ¬´{selected_category}¬ª")

# –ü–æ–¥—Å–∫–∞–∑–∫–∞, –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
elif "filtered_results" in st.session_state and not st.session_state.filtered_results:
    st.warning("–ù–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –≤—ã–±—Ä–∞–Ω–Ω–æ–π —Ä—É–±—Ä–∏–∫–µ.")